{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c5d3b8",
   "metadata": {},
   "source": [
    "# A gentle introduction to autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f35984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5561bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a9c71",
   "metadata": {},
   "source": [
    "random datapoint and random label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63088f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand(1, 3, 64, 64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d5dacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.rand(1, 1000)\n",
    "len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f8ddee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.4540e-01, -5.7048e-01, -6.7406e-01, -1.8693e+00, -8.6959e-01,\n",
       "         -2.5654e-01, -7.7244e-01,  4.8245e-01,  5.7862e-01, -6.7575e-01,\n",
       "         -7.8480e-01, -8.4431e-01, -1.6934e-01, -9.1002e-01, -1.2424e+00,\n",
       "         -5.0489e-01, -8.2507e-01, -3.1685e-01, -2.8090e-01, -5.0147e-01,\n",
       "         -1.2400e+00, -6.6627e-01, -1.2883e+00,  1.3171e-01, -1.0650e+00,\n",
       "         -1.3272e+00, -8.7364e-01, -1.2918e+00, -1.1954e+00, -4.2201e-01,\n",
       "         -8.7971e-01, -8.4451e-01, -5.1901e-01, -9.6086e-01, -5.5852e-01,\n",
       "         -5.7839e-01,  4.5073e-01, -9.6552e-01, -6.5845e-01,  6.7949e-02,\n",
       "         -7.0722e-01, -8.6755e-01, -1.2081e+00, -2.3011e-01, -6.1143e-01,\n",
       "         -4.7931e-01, -9.6323e-01, -2.4569e-01, -1.1342e+00, -1.1293e+00,\n",
       "         -6.1079e-01,  4.1723e-01, -3.2438e-01, -7.2536e-01, -4.3572e-01,\n",
       "         -1.3641e+00, -4.4534e-01, -1.5375e+00, -7.0851e-01, -5.2085e-01,\n",
       "          7.3145e-01,  1.2916e-01, -1.7046e-01, -1.5844e-01, -1.0370e+00,\n",
       "         -6.1310e-01, -4.0188e-01, -3.6641e-01, -9.3970e-01, -1.1815e+00,\n",
       "         -1.6563e+00, -2.5140e-01, -1.8143e+00, -7.7184e-01, -1.6580e+00,\n",
       "         -1.8399e+00, -2.6728e-01, -9.7077e-01,  1.8959e-02, -4.6951e-02,\n",
       "         -4.6524e-01, -1.3343e+00,  1.0458e-01, -3.0076e-01, -6.9740e-01,\n",
       "         -2.5865e-01,  3.2824e-01,  1.2536e-01, -3.2566e-01, -9.5439e-01,\n",
       "         -1.3026e+00, -9.6184e-01, -1.8248e+00, -3.5789e-01,  3.7494e-01,\n",
       "         -2.0117e+00, -9.6147e-01, -3.0131e-01, -1.2687e+00, -1.6204e-01,\n",
       "         -1.2309e+00, -7.4642e-01, -8.1143e-01, -2.5866e-01, -4.4735e-01,\n",
       "         -1.0312e+00, -6.7817e-01, -1.6441e+00, -1.3806e+00, -1.7451e+00,\n",
       "         -1.4967e+00, -7.3365e-01,  1.2889e+00,  3.1434e-01,  1.4265e-01,\n",
       "         -1.2919e+00, -1.0630e+00, -4.9948e-01,  5.2276e-01, -7.1118e-01,\n",
       "         -9.2598e-01, -2.2275e-01,  2.4669e-01, -5.2848e-02,  9.1833e-01,\n",
       "         -4.5529e-01,  1.3654e-02, -1.4431e+00, -1.2953e+00, -1.0723e+00,\n",
       "         -1.5771e+00, -1.8120e+00, -1.1097e+00, -1.2294e+00, -6.5002e-01,\n",
       "         -1.4012e+00, -8.1224e-01, -1.0138e+00, -1.6263e+00, -1.3801e+00,\n",
       "         -1.5099e+00, -1.6114e+00, -2.0072e+00, -1.3883e+00, -6.3454e-01,\n",
       "         -5.1480e-01, -9.2623e-01, -1.9013e+00, -9.4675e-01, -1.4873e+00,\n",
       "          4.8058e-01,  1.7752e+00, -6.0545e-01, -3.6385e-01,  2.0692e-01,\n",
       "          5.1215e-02, -2.0937e-01,  8.5011e-02,  1.4632e-01,  1.0096e-01,\n",
       "          4.0961e-01,  7.1269e-01,  1.8908e-01,  4.7410e-01,  2.5855e-01,\n",
       "         -2.5358e-01, -3.0560e-01, -4.6573e-01,  6.4572e-01, -1.1639e-02,\n",
       "         -3.5186e-01,  7.1740e-01,  5.6551e-01,  5.5580e-01,  3.5779e-01,\n",
       "         -8.4691e-01,  2.6501e-01, -1.0394e-01,  2.9099e-01,  5.4149e-01,\n",
       "          5.1628e-01, -2.0170e-01,  3.6844e-01, -5.2069e-02,  5.0299e-01,\n",
       "          7.1342e-01,  6.2876e-01, -2.0531e-02, -1.3920e-01,  2.8377e-01,\n",
       "         -4.7113e-01, -5.7500e-02,  1.6416e-01,  6.0122e-01, -6.0332e-01,\n",
       "          8.3838e-01, -1.7674e-02, -4.7275e-02,  3.9530e-02,  6.8729e-01,\n",
       "          5.4358e-02, -1.4678e-01,  3.1714e-01,  6.0218e-01,  1.3140e-01,\n",
       "          4.0471e-01,  1.4016e-01,  5.4210e-01,  1.3380e+00,  5.6327e-01,\n",
       "         -1.6130e-01,  3.4855e-01,  1.7488e-01, -9.9544e-02, -1.7555e-01,\n",
       "          3.5144e-01,  5.2172e-02,  2.3735e-01, -2.5466e-01,  5.8269e-01,\n",
       "          3.0276e-01, -7.3050e-02,  2.9704e-01,  1.0656e+00,  2.5230e-01,\n",
       "          3.1122e-01,  7.1134e-02,  9.0491e-01, -3.5411e-01, -1.6573e-01,\n",
       "         -2.8670e-02,  6.1680e-01,  6.6797e-01, -2.8553e-01,  4.8243e-01,\n",
       "          7.8848e-01,  3.6294e-01,  3.5838e-01,  7.9585e-01, -1.4346e-01,\n",
       "          6.1731e-01, -2.0236e-01,  2.9915e-01,  1.2339e-02, -1.3562e-01,\n",
       "          3.9849e-01,  4.1117e-01,  1.2461e-01,  8.7292e-01,  3.6227e-01,\n",
       "          6.1713e-01,  5.3549e-01, -9.2151e-01,  1.0333e+00,  7.5743e-01,\n",
       "         -6.0625e-01,  4.4766e-01,  3.2038e-01,  9.0608e-03,  3.8792e-01,\n",
       "         -3.7041e-01, -6.9348e-01, -2.5052e-01,  7.3138e-01,  1.2143e+00,\n",
       "          8.0287e-01,  3.7069e-01,  6.9376e-01, -1.5726e-01, -3.8384e-01,\n",
       "         -8.3862e-01, -9.2437e-01, -4.7776e-01,  4.3833e-01, -9.5186e-01,\n",
       "         -1.2689e+00, -1.4890e+00, -7.1432e-01, -1.1833e+00, -4.3565e-01,\n",
       "         -3.5370e-01,  1.1642e+00,  1.0020e+00,  7.3221e-02,  1.5282e-01,\n",
       "          1.1349e+00, -6.4785e-01, -3.1405e-01, -1.0658e+00, -1.9158e+00,\n",
       "         -1.2001e+00, -1.6163e+00, -3.9525e-01, -1.1767e+00, -1.0226e+00,\n",
       "         -1.0452e+00, -1.0025e+00, -1.5513e+00, -3.2721e-01, -2.2712e-01,\n",
       "         -1.9612e+00, -8.7373e-01, -6.7658e-01, -4.9104e-01, -1.3732e+00,\n",
       "         -1.1235e+00, -9.3398e-03, -8.5227e-01, -1.3597e+00, -8.6067e-01,\n",
       "          1.1249e-02, -2.7867e-03, -1.0695e-01,  5.2202e-02,  5.2413e-01,\n",
       "         -4.9812e-01, -9.1177e-01, -1.1115e+00, -1.2894e+00, -1.0691e+00,\n",
       "         -1.8018e+00, -1.0899e+00, -1.5413e+00, -1.7434e+00, -1.7925e+00,\n",
       "         -1.7346e+00, -1.4869e+00, -3.3050e-01, -4.5866e-01, -5.0690e-01,\n",
       "          1.1692e-01, -4.8069e-02,  5.7052e-02,  3.4396e-01, -5.6601e-01,\n",
       "         -8.2530e-01, -1.5135e+00,  1.7828e-01,  7.6529e-01, -1.1090e+00,\n",
       "         -3.5469e-01,  6.4691e-01, -4.2511e-01, -1.3933e+00, -6.9636e-01,\n",
       "          6.5865e-01, -8.5540e-01, -1.6621e+00, -5.8033e-02, -1.2502e+00,\n",
       "         -1.2068e+00, -2.2468e+00, -1.3896e+00, -8.0009e-01, -6.1056e-01,\n",
       "          4.4001e-01,  1.0461e+00,  3.7981e-02,  4.2787e-01,  4.8184e-01,\n",
       "         -2.1382e-01,  6.5508e-01,  1.2276e-01,  1.2212e-01, -4.1012e-01,\n",
       "         -9.5361e-01, -1.4472e+00, -6.8672e-01, -1.0044e+00, -8.2639e-01,\n",
       "         -7.4549e-01, -3.6238e-01, -4.3708e-01, -2.1793e-01, -5.3523e-01,\n",
       "         -1.4094e+00, -1.2576e+00,  3.2988e-01, -4.3554e-01, -8.2601e-01,\n",
       "          3.0404e-01, -6.9229e-01, -5.1627e-01, -9.4859e-01, -8.2468e-01,\n",
       "         -5.7521e-01, -9.9094e-01, -1.0630e+00, -1.2669e+00, -1.9723e-01,\n",
       "         -2.1260e-02,  1.9073e-01, -1.6536e+00, -1.8548e+00, -1.9195e-01,\n",
       "          4.8971e-01, -1.6346e+00, -9.8574e-01,  7.7167e-01,  1.1346e-01,\n",
       "         -6.4455e-01,  8.6414e-01,  1.9237e-01, -2.0851e+00, -1.7016e+00,\n",
       "         -6.4177e-01, -3.5201e-01, -6.8077e-02,  9.3480e-02,  7.8259e-01,\n",
       "         -3.8996e-01,  3.2230e-01,  2.0411e+00,  7.3549e-01,  6.1659e-01,\n",
       "          8.9787e-01, -1.9552e-01,  5.3576e-01,  6.0685e-01,  1.0418e+00,\n",
       "          9.4234e-01,  1.3695e+00, -1.5177e-01,  4.1389e-01,  5.5484e-01,\n",
       "         -1.0006e+00, -1.4569e-01,  1.3119e+00,  1.7378e+00,  7.3991e-01,\n",
       "         -6.3281e-01,  3.2675e-01,  3.9694e-01,  1.0558e+00,  9.0117e-01,\n",
       "          1.3564e+00, -3.2801e-01, -2.9156e-01,  3.5549e-01,  4.1331e-01,\n",
       "          1.1501e+00,  5.6229e-01, -1.9436e-02, -3.4542e-01, -3.3603e-01,\n",
       "          9.6717e-02,  5.4644e-01,  1.6060e+00,  9.9609e-01, -2.3856e-01,\n",
       "          3.1297e-01,  3.1389e-01,  7.8341e-01,  1.5130e-01, -1.4756e-01,\n",
       "          7.4301e-01,  1.7681e+00,  1.2288e+00, -1.8096e-01,  9.8067e-01,\n",
       "         -7.1264e-01,  2.9990e-01,  1.5256e+00,  2.6880e+00,  1.0928e+00,\n",
       "         -9.2967e-02, -1.3001e+00, -1.5983e-01,  7.3413e-02,  1.5781e+00,\n",
       "          9.3620e-01,  4.4968e-01,  2.9073e-01,  8.7849e-01, -2.2619e-01,\n",
       "          2.1093e-01,  1.4602e-01,  9.0437e-01,  9.7167e-01,  3.5998e-01,\n",
       "          1.3552e-01,  2.6143e-01,  7.7978e-02, -8.1005e-01, -1.1638e+00,\n",
       "         -1.6299e-01, -3.3247e-02,  1.1391e+00,  1.6327e+00,  1.2326e+00,\n",
       "          4.0438e-01,  6.9227e-01,  6.6384e-01, -1.1441e+00,  1.0867e+00,\n",
       "         -9.6426e-01, -5.9829e-02, -4.6981e-01,  1.6120e-01,  1.1839e+00,\n",
       "         -1.7141e+00,  5.2905e-01,  1.2481e+00,  5.8615e-01,  1.4025e+00,\n",
       "          1.0542e+00,  1.0047e+00,  5.7986e-01,  7.6805e-01,  4.0528e-01,\n",
       "         -1.0574e+00, -7.7274e-01,  7.6123e-01,  3.0041e-01,  1.1563e+00,\n",
       "          1.8543e+00,  7.9345e-01,  1.6678e-01,  1.3916e+00,  7.4812e-01,\n",
       "         -4.1679e-01,  1.5768e-01,  1.0349e+00,  1.9974e+00,  1.5668e-01,\n",
       "         -6.8038e-01,  2.2003e-01, -2.9454e-01,  1.3887e-01,  3.4342e-01,\n",
       "          9.6797e-01,  2.4976e-01, -6.2519e-02, -7.6245e-01,  3.5986e-01,\n",
       "         -6.7080e-01, -2.7505e-01, -4.6136e-01, -1.4231e-01,  1.3249e+00,\n",
       "         -1.1607e+00,  1.5684e+00,  1.2009e+00,  9.4549e-01,  2.0090e-01,\n",
       "          6.2596e-01,  5.4503e-01, -1.9767e+00, -9.9977e-01, -1.4281e-01,\n",
       "         -7.1307e-01,  5.5505e-01,  7.3781e-01, -1.9532e-01, -1.3101e+00,\n",
       "         -3.5204e-01,  1.3412e-01,  5.0170e-01,  1.3442e+00,  1.0025e+00,\n",
       "          2.0027e-01, -1.4481e-01,  5.3697e-01,  2.2759e-01, -1.0276e+00,\n",
       "         -5.5392e-01,  3.9270e-02,  6.4868e-01,  7.1451e-01, -3.6072e-01,\n",
       "          9.2498e-01,  1.8361e-01,  8.4255e-01, -5.3879e-01,  4.7262e-01,\n",
       "         -1.5924e-01, -7.2718e-01,  9.0926e-01,  3.8408e-01,  4.0246e-01,\n",
       "         -2.5985e-01, -3.5708e-01,  1.1100e+00,  7.4210e-01,  9.1015e-01,\n",
       "          9.9411e-01, -5.4061e-01,  1.7814e+00,  1.2154e+00,  1.3727e+00,\n",
       "         -4.4283e-01,  6.3047e-01, -5.6239e-01,  9.8662e-01,  3.5818e-01,\n",
       "         -7.7430e-01,  8.4293e-01, -4.1244e-03, -4.6471e-01,  5.4898e-01,\n",
       "          2.1716e+00, -2.0777e-01,  1.0155e-01, -3.5460e-01,  4.7150e-01,\n",
       "          4.5120e-01,  1.3223e+00, -5.8651e-01,  8.0770e-01, -7.8424e-02,\n",
       "          9.1291e-01,  8.0143e-01, -6.5902e-01,  8.0927e-01,  2.5009e-01,\n",
       "          3.6158e-01,  1.1081e+00,  4.6966e-01,  1.8770e+00,  6.9961e-01,\n",
       "          1.1283e+00,  6.2162e-01,  2.4597e-01,  3.7903e-01,  3.1934e-01,\n",
       "         -8.3047e-01,  1.2378e+00, -3.1705e-01, -1.0906e+00,  6.9744e-01,\n",
       "         -6.6944e-02,  1.0842e+00,  4.3006e-01,  1.1418e+00, -2.5238e-01,\n",
       "          2.5202e-01,  1.3391e+00,  8.4973e-01,  7.6332e-01,  3.1182e-01,\n",
       "         -1.8669e+00,  1.3338e+00, -2.6519e-01,  1.1639e+00,  6.8587e-01,\n",
       "         -8.0286e-01,  9.0634e-01,  5.0900e-01, -2.6048e-01, -1.5666e+00,\n",
       "          1.0075e+00,  1.5901e-01,  8.6541e-01,  9.0251e-01,  2.1575e-01,\n",
       "          5.7394e-01, -1.0992e-01,  4.2098e-02,  2.6135e-01,  6.8177e-01,\n",
       "          2.3567e-01, -9.6845e-01,  1.8550e-03, -9.1528e-01,  1.2278e+00,\n",
       "          2.0376e-02,  1.4180e+00,  4.6215e-01, -9.9528e-01, -5.6263e-01,\n",
       "          4.3540e-01, -5.2811e-01, -5.4707e-01,  8.0895e-01,  1.5028e+00,\n",
       "         -6.3489e-01,  1.6458e+00,  1.0633e+00,  1.2425e+00, -7.9607e-02,\n",
       "          8.4196e-01,  6.9466e-01, -6.3518e-01,  6.0182e-01,  9.7142e-01,\n",
       "         -1.6287e+00, -2.1617e-01, -1.1362e+00, -1.1149e-01, -8.1004e-01,\n",
       "         -5.7226e-01,  8.5534e-01,  8.6847e-01,  8.1375e-01, -9.2704e-01,\n",
       "          7.4813e-01,  1.7110e+00, -6.3153e-03, -3.1478e-01,  6.8163e-01,\n",
       "          2.0642e+00, -4.6597e-01,  4.4092e-03,  4.7354e-01,  6.3424e-01,\n",
       "         -2.7434e-01, -3.0968e-01,  3.0730e-01,  5.7438e-01,  3.2569e-01,\n",
       "          1.0438e+00,  9.5272e-01, -1.1821e-01, -3.3608e-01,  4.0995e-01,\n",
       "         -6.4318e-01,  6.5484e-01, -3.0892e-01, -3.8598e-01,  8.4698e-01,\n",
       "          4.7051e-01,  2.3305e-01,  1.6785e+00,  3.2601e-01, -5.7222e-01,\n",
       "          1.2471e+00, -8.4998e-01, -3.6293e-01,  1.6245e+00, -4.2556e-01,\n",
       "          2.1160e-01,  2.3377e+00, -4.8571e-01,  2.0613e+00, -1.4152e+00,\n",
       "          1.5654e-01,  1.2939e-01,  9.9467e-01,  7.4142e-01,  2.2490e-02,\n",
       "          1.3154e+00,  1.2505e-01,  6.0596e-01,  1.6359e-01,  8.1042e-01,\n",
       "          5.6023e-02,  1.7249e-01,  9.6982e-01,  7.7762e-01,  1.2981e+00,\n",
       "          8.2401e-01,  9.0902e-02,  1.9376e-01,  7.5542e-01,  8.4327e-01,\n",
       "         -6.8916e-01,  1.1479e+00,  2.9283e-01,  1.2562e+00, -2.2957e-01,\n",
       "          4.6357e-01,  1.0280e+00,  5.2741e-01,  7.3222e-01,  1.3763e+00,\n",
       "          1.0770e+00,  2.7751e-01,  7.2773e-01, -2.9644e-02,  1.4332e+00,\n",
       "          4.4420e-01,  2.2241e-01,  1.5484e+00,  8.6615e-01,  6.1592e-01,\n",
       "          4.1442e-01,  4.4736e-01,  6.3443e-01,  1.1568e+00, -5.7388e-01,\n",
       "         -7.1855e-01, -5.4549e-01,  1.3990e+00,  8.0586e-01,  2.0448e+00,\n",
       "          3.3448e-01,  4.6150e-01,  1.4395e+00,  3.2128e-01,  3.9101e-02,\n",
       "          1.1091e+00,  1.1913e+00,  1.6282e+00,  1.0567e+00,  6.5956e-01,\n",
       "          2.5961e-01,  9.8046e-01,  1.0882e+00, -6.0329e-01,  6.5199e-01,\n",
       "         -7.5088e-01,  9.2067e-02, -9.7401e-01, -9.5679e-01,  1.0137e+00,\n",
       "          1.0306e+00,  3.6433e-01,  1.1847e-01,  1.5954e+00,  1.5087e-01,\n",
       "         -4.2920e-01,  1.0835e+00, -5.4919e-03,  1.5371e+00, -5.6648e-01,\n",
       "         -7.7481e-01,  2.1964e-01, -1.1377e+00,  1.8345e+00,  4.0930e-01,\n",
       "         -1.5263e+00, -1.2556e+00,  3.9573e-01,  7.7008e-01,  9.8286e-01,\n",
       "         -7.5970e-01,  4.7186e-03,  7.9245e-01,  9.9562e-01, -2.6452e-01,\n",
       "          1.3200e+00,  5.6481e-01, -7.1869e-01, -8.8226e-01,  9.8728e-02,\n",
       "          5.4966e-01,  1.4918e+00,  1.4587e+00,  1.0567e+00, -6.9724e-01,\n",
       "          1.5627e+00,  6.0486e-01,  4.8822e-01,  7.6489e-01,  7.2559e-01,\n",
       "          1.5529e+00,  5.6806e-01, -4.1617e-01,  1.8684e-01,  9.0014e-01,\n",
       "          1.2158e+00,  1.6874e+00,  1.8032e+00, -6.5562e-01, -2.6949e-01,\n",
       "          1.1209e+00, -6.4205e-01, -2.3820e-01,  1.7960e-03,  9.8637e-01,\n",
       "          2.5600e-01,  1.6280e+00,  1.1506e+00, -3.6888e-02, -1.5657e-01,\n",
       "          7.8216e-01,  6.3116e-02,  6.4211e-02,  1.6078e+00, -4.3077e-01,\n",
       "          7.6137e-01, -1.2379e+00,  1.1762e+00, -1.3455e+00, -1.9637e+00,\n",
       "          5.0977e-01,  1.9195e+00, -3.9412e-01, -2.2265e-01,  1.2675e+00,\n",
       "          8.8083e-01, -1.7905e-02,  1.1948e+00,  1.2526e+00, -3.5018e-01,\n",
       "          1.0722e-01, -8.6322e-02, -1.4433e-01, -9.7583e-01,  4.3580e-01,\n",
       "         -9.8749e-02, -2.0159e-02,  8.5525e-01,  3.1843e-01, -4.9205e-01,\n",
       "         -5.3640e-01,  1.2847e+00,  5.1799e-01,  2.1509e+00,  2.1063e+00,\n",
       "         -8.1485e-01, -5.7165e-01,  1.7472e+00,  7.0757e-01,  1.2379e+00,\n",
       "          3.8059e-01, -5.9225e-01,  1.3134e+00, -7.5342e-01,  8.0733e-01,\n",
       "          1.2072e+00,  1.0867e+00,  8.8675e-01, -4.3190e-01, -1.4565e+00,\n",
       "         -4.1970e-01,  2.5493e-01,  3.0207e-01,  9.3168e-01,  3.2917e-01,\n",
       "          2.4221e-02,  1.0784e+00, -4.9044e-01,  6.1124e-01, -1.6750e-01,\n",
       "         -1.0135e+00, -1.2792e+00, -5.9228e-01, -8.4432e-02,  1.7143e+00,\n",
       "         -3.9189e-01, -2.5270e-02,  3.5975e-01, -1.8527e+00,  1.1634e-01,\n",
       "         -4.9729e-01,  5.1673e-01,  2.8954e-01, -3.6678e-02, -1.0524e-02,\n",
       "         -5.7067e-01, -9.4510e-01, -5.4551e-01,  3.3799e-01, -6.3647e-01,\n",
       "         -7.5930e-01, -1.2811e+00,  2.5711e-01,  5.9428e-01, -4.4220e-01,\n",
       "         -8.5972e-02, -6.8691e-01, -8.4290e-01, -1.8027e-01,  5.2583e-01,\n",
       "         -6.6519e-01, -6.0968e-01, -6.8687e-01,  7.3431e-02, -1.0409e+00,\n",
       "          1.9392e-01,  2.0813e-01, -3.1502e-01, -1.0595e+00, -1.3252e+00,\n",
       "         -3.0285e-01,  4.7497e-01, -7.0775e-01,  1.0133e+00,  3.2782e-02,\n",
       "         -1.7926e-01,  9.3826e-01, -3.1292e-01, -6.7484e-01, -2.0930e+00,\n",
       "          1.1319e+00, -1.5765e+00,  6.0267e-01,  3.3807e-01, -7.6331e-01,\n",
       "         -6.8359e-01,  2.8437e-02,  7.0713e-01, -7.7049e-01, -7.1954e-01,\n",
       "         -1.4984e+00, -2.8458e+00,  1.3100e+00, -2.7988e-01, -8.4343e-01,\n",
       "         -1.3156e-01, -1.3861e+00, -1.1484e+00, -2.1225e+00, -1.2843e+00,\n",
       "         -7.5394e-01, -1.2456e-02, -9.8263e-01,  1.1409e+00,  1.2707e+00]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "prediction = model(data)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fcde3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check grad after backward (should have values)\n",
    "print(list(model.parameters())[0].grad)  # Tensor with gradient values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25bc409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-500.3949, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e21a0dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a98653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8acf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.8034e-04, -4.3571e-04, -6.0502e-04,  ...,  1.4827e-04,\n",
      "           -6.7896e-04,  6.9517e-04],\n",
      "          [-2.6247e-04,  1.0883e-03, -1.1197e-03,  ..., -5.0035e-04,\n",
      "            2.7686e-04, -4.4323e-04],\n",
      "          [ 3.5893e-04,  4.2283e-04,  3.1428e-04,  ..., -7.0000e-04,\n",
      "           -1.4655e-05,  2.6408e-04],\n",
      "          ...,\n",
      "          [ 1.3935e-05,  2.9207e-04,  1.1957e-03,  ...,  3.1835e-04,\n",
      "           -7.7647e-04,  1.8239e-04],\n",
      "          [ 7.5836e-06, -4.3775e-04, -6.2779e-04,  ...,  7.9577e-05,\n",
      "           -5.7791e-04, -2.1012e-04],\n",
      "          [ 1.6683e-04,  6.8008e-04, -9.2604e-04,  ..., -4.3892e-04,\n",
      "           -5.1387e-04, -3.9908e-04]],\n",
      "\n",
      "         [[-2.1648e-04, -3.3837e-04, -4.9625e-04,  ..., -1.0410e-03,\n",
      "            1.8463e-04, -2.0342e-04],\n",
      "          [-3.4355e-04,  3.1044e-04, -3.0627e-04,  ...,  9.9713e-05,\n",
      "           -7.5910e-05,  8.9708e-04],\n",
      "          [-1.9173e-04, -8.2307e-04,  5.1731e-04,  ..., -8.8237e-05,\n",
      "            6.0596e-04,  4.1310e-04],\n",
      "          ...,\n",
      "          [-1.3832e-04, -1.2922e-03,  5.1989e-04,  ...,  3.4376e-04,\n",
      "           -2.3870e-04, -5.6079e-04],\n",
      "          [ 8.2529e-05,  4.7345e-04, -2.3592e-04,  ..., -1.9674e-04,\n",
      "            6.0131e-04,  1.2177e-03],\n",
      "          [ 2.0175e-04, -4.1639e-04, -4.0182e-04,  ...,  7.0626e-05,\n",
      "            6.9470e-05, -1.3052e-04]],\n",
      "\n",
      "         [[ 7.1443e-04,  3.1691e-04,  7.8342e-05,  ..., -2.6744e-05,\n",
      "           -7.4511e-04,  3.3597e-05],\n",
      "          [-1.3045e-04,  9.5838e-04, -6.6306e-05,  ..., -2.9543e-04,\n",
      "            4.6916e-04,  7.8069e-05],\n",
      "          [-2.9930e-04,  2.1062e-04, -7.1912e-04,  ...,  2.4195e-04,\n",
      "           -6.2167e-04, -9.8923e-06],\n",
      "          ...,\n",
      "          [ 5.6513e-04, -4.2613e-04, -6.0234e-05,  ...,  6.7839e-04,\n",
      "           -5.6547e-04,  6.7005e-04],\n",
      "          [-1.6288e-04, -3.6872e-04,  8.4744e-04,  ...,  4.3829e-04,\n",
      "           -2.9998e-04, -8.3428e-05],\n",
      "          [-1.0510e-04,  1.6888e-04,  4.9950e-04,  ..., -4.0371e-04,\n",
      "            3.7621e-04, -8.3670e-05]]],\n",
      "\n",
      "\n",
      "        [[[-6.2504e-04, -9.4507e-04,  1.2798e-03,  ...,  2.2524e-04,\n",
      "            4.6137e-04,  1.2713e-03],\n",
      "          [ 4.9057e-04, -2.6428e-05,  5.7543e-05,  ...,  2.5272e-04,\n",
      "           -2.2960e-04, -7.8765e-04],\n",
      "          [ 4.3728e-04, -1.5686e-03, -1.4215e-03,  ...,  5.5190e-04,\n",
      "           -1.1459e-03,  1.8589e-04],\n",
      "          ...,\n",
      "          [ 4.9713e-04,  5.1155e-04,  1.7542e-04,  ...,  7.4162e-04,\n",
      "           -4.2771e-04, -8.5935e-04],\n",
      "          [-2.1654e-04,  3.0030e-04, -8.4900e-05,  ...,  5.7694e-04,\n",
      "            4.5282e-04,  7.5005e-04],\n",
      "          [ 1.8194e-04,  1.0253e-03, -3.9325e-04,  ..., -1.9986e-04,\n",
      "            4.7005e-04,  8.9436e-04]],\n",
      "\n",
      "         [[ 9.3632e-04, -1.0776e-03, -5.1521e-04,  ..., -5.7647e-04,\n",
      "           -7.9463e-04, -1.5255e-06],\n",
      "          [ 8.2643e-04, -1.0110e-03,  1.6150e-04,  ..., -8.7926e-04,\n",
      "           -2.7596e-04, -1.9098e-04],\n",
      "          [ 1.2471e-04,  1.5465e-05, -1.3697e-03,  ...,  7.7550e-04,\n",
      "            1.2653e-04, -9.5963e-04],\n",
      "          ...,\n",
      "          [-2.9046e-04, -1.5909e-03, -9.6608e-04,  ...,  1.2876e-03,\n",
      "            9.7050e-04, -1.2630e-03],\n",
      "          [-2.2284e-05,  7.9113e-04,  9.5756e-04,  ...,  3.4995e-05,\n",
      "           -3.0245e-04,  4.1400e-04],\n",
      "          [-2.9813e-04,  4.2235e-04, -5.6224e-04,  ..., -1.0002e-03,\n",
      "            6.2496e-04,  1.8046e-03]],\n",
      "\n",
      "         [[-1.4170e-03, -1.6702e-03,  1.9143e-04,  ..., -1.1904e-03,\n",
      "           -5.6300e-05, -4.3669e-05],\n",
      "          [-4.9878e-04, -4.4576e-04,  9.2321e-04,  ...,  5.8594e-04,\n",
      "            1.5455e-03,  3.2066e-04],\n",
      "          [ 5.4886e-04, -1.0712e-03,  8.4120e-05,  ...,  1.9340e-04,\n",
      "           -5.8342e-04, -5.9731e-04],\n",
      "          ...,\n",
      "          [ 1.4032e-03, -1.0653e-03, -3.2089e-04,  ...,  3.1069e-05,\n",
      "           -1.0493e-03, -4.1035e-04],\n",
      "          [ 2.1226e-03,  9.3286e-04,  1.0285e-03,  ...,  1.7491e-03,\n",
      "            1.3020e-03,  1.4805e-03],\n",
      "          [ 1.1838e-03, -5.8181e-04,  7.7180e-04,  ..., -1.8115e-04,\n",
      "            1.0854e-03, -1.0765e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.2643e-03,  2.4839e-03,  2.3614e-04,  ...,  2.8257e-03,\n",
      "            2.0617e-03, -1.2223e-03],\n",
      "          [-9.1906e-04, -3.2191e-03, -4.5306e-04,  ..., -2.8361e-03,\n",
      "           -2.1114e-03, -2.2222e-03],\n",
      "          [-2.6445e-03,  4.2382e-03,  2.0228e-03,  ..., -7.9866e-04,\n",
      "            3.5320e-04, -2.2560e-04],\n",
      "          ...,\n",
      "          [-2.8990e-03,  1.0241e-03,  5.4493e-04,  ..., -5.0711e-04,\n",
      "           -4.5947e-03, -1.6901e-03],\n",
      "          [ 2.1264e-03,  1.6665e-03,  1.7362e-03,  ...,  4.4840e-03,\n",
      "           -1.4555e-04, -1.0502e-03],\n",
      "          [ 9.6427e-04,  1.0335e-03, -2.4813e-03,  ...,  3.4411e-03,\n",
      "           -1.0978e-04, -2.2474e-03]],\n",
      "\n",
      "         [[ 5.7913e-04,  3.6940e-03,  3.0986e-03,  ...,  1.2539e-03,\n",
      "           -1.6109e-05, -1.1749e-03],\n",
      "          [ 8.5821e-04, -1.4932e-04, -1.4790e-03,  ...,  2.0508e-03,\n",
      "            5.3923e-03,  1.9196e-03],\n",
      "          [ 1.6076e-03, -5.5017e-03,  1.2844e-03,  ..., -2.6661e-04,\n",
      "           -2.4010e-03,  1.6889e-03],\n",
      "          ...,\n",
      "          [ 3.9232e-03, -1.1068e-03, -1.7552e-05,  ...,  2.5508e-04,\n",
      "            7.6647e-04,  1.9993e-03],\n",
      "          [ 1.8285e-03,  3.2040e-03, -1.6676e-03,  ..., -1.3071e-03,\n",
      "           -3.4983e-03, -6.9670e-04],\n",
      "          [ 1.9966e-03,  7.3926e-04, -4.7012e-03,  ...,  1.1282e-03,\n",
      "            6.2744e-03, -2.3632e-03]],\n",
      "\n",
      "         [[ 2.1931e-04, -3.1638e-04,  1.4572e-03,  ...,  5.1058e-04,\n",
      "            5.4688e-04,  5.0293e-03],\n",
      "          [-3.0539e-03, -3.7258e-03, -2.1874e-03,  ...,  1.0306e-03,\n",
      "           -1.6111e-04,  3.5823e-03],\n",
      "          [ 3.2788e-03, -1.1266e-03, -2.0215e-03,  ..., -5.3583e-04,\n",
      "            1.7731e-03,  8.5249e-04],\n",
      "          ...,\n",
      "          [ 3.6919e-04,  1.6602e-03, -3.6147e-04,  ..., -4.0885e-03,\n",
      "            1.3087e-03,  9.9423e-04],\n",
      "          [ 3.2337e-03, -1.5657e-03,  1.9780e-03,  ..., -1.8493e-03,\n",
      "            1.6493e-04, -1.8593e-03],\n",
      "          [-1.5065e-03, -3.8783e-04, -1.1477e-03,  ..., -5.7889e-03,\n",
      "           -6.6364e-04, -2.6214e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.4304e-04, -1.9884e-03,  8.0010e-04,  ..., -4.6899e-04,\n",
      "            2.0433e-04,  2.2170e-03],\n",
      "          [-2.5961e-04, -1.0669e-03, -1.9531e-03,  ...,  6.7792e-04,\n",
      "            2.3297e-04,  1.5317e-03],\n",
      "          [ 5.0638e-04, -1.7676e-04, -2.5275e-04,  ..., -6.9517e-04,\n",
      "            5.1588e-04, -2.0353e-06],\n",
      "          ...,\n",
      "          [ 8.7058e-04,  4.8820e-04, -4.2483e-04,  ..., -9.2081e-05,\n",
      "           -4.9529e-04, -3.0998e-03],\n",
      "          [ 1.0819e-03, -7.8343e-04, -2.1164e-04,  ...,  1.0956e-03,\n",
      "            2.3008e-03, -5.5857e-06],\n",
      "          [-7.3441e-04,  1.0178e-03, -4.9806e-04,  ..., -2.3712e-03,\n",
      "            4.0825e-04,  2.7417e-03]],\n",
      "\n",
      "         [[-1.1150e-03,  2.7287e-03, -2.0474e-04,  ...,  4.6928e-04,\n",
      "            2.8308e-04, -4.5191e-04],\n",
      "          [-1.1720e-03, -2.6394e-04, -1.0212e-04,  ...,  3.2767e-04,\n",
      "            5.2777e-05,  1.9905e-03],\n",
      "          [ 1.4900e-03,  5.9279e-04,  6.5132e-04,  ...,  2.2350e-03,\n",
      "            1.6593e-03,  1.4040e-03],\n",
      "          ...,\n",
      "          [-1.3526e-03, -7.1822e-05, -7.2275e-04,  ...,  2.2097e-03,\n",
      "           -2.3549e-04,  9.5422e-04],\n",
      "          [-2.0638e-03, -2.4154e-04, -1.5066e-03,  ...,  1.0613e-03,\n",
      "            7.3698e-04,  1.6909e-03],\n",
      "          [-8.5559e-04,  9.3749e-04, -9.9931e-04,  ..., -9.7602e-04,\n",
      "            9.0972e-06,  4.4311e-05]],\n",
      "\n",
      "         [[ 1.3932e-03, -3.9177e-04,  5.8802e-04,  ...,  2.3084e-04,\n",
      "            6.6730e-04,  1.3803e-03],\n",
      "          [ 7.2054e-04, -4.3938e-04, -6.5026e-04,  ...,  1.2931e-03,\n",
      "            3.4145e-04,  7.4997e-05],\n",
      "          [ 3.7006e-05,  9.3454e-04,  2.9092e-03,  ...,  1.2469e-03,\n",
      "            3.9404e-04, -1.8973e-04],\n",
      "          ...,\n",
      "          [ 5.6056e-04, -8.3354e-04,  1.3813e-03,  ...,  1.8303e-03,\n",
      "           -1.2836e-04,  9.0514e-04],\n",
      "          [-1.1495e-04,  2.9776e-04, -6.1730e-04,  ..., -3.4527e-04,\n",
      "            7.9742e-04, -4.6539e-04],\n",
      "          [ 6.6745e-04,  5.5766e-04,  6.7184e-05,  ..., -1.1576e-03,\n",
      "            1.4509e-04, -1.0905e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5504e-04,  9.7387e-05, -5.4805e-04,  ..., -1.0950e-03,\n",
      "            3.9306e-04, -1.4552e-04],\n",
      "          [-5.2077e-04, -4.7597e-05, -4.8618e-04,  ..., -2.9466e-04,\n",
      "           -1.3825e-04, -2.6331e-04],\n",
      "          [ 3.1911e-04, -1.1372e-03, -5.2919e-04,  ..., -5.2110e-04,\n",
      "           -1.0491e-03, -3.4506e-05],\n",
      "          ...,\n",
      "          [-6.5357e-04, -1.2620e-03,  8.9470e-04,  ...,  1.4544e-03,\n",
      "            2.0717e-04, -9.2423e-04],\n",
      "          [-1.9503e-04, -1.2488e-03, -1.1124e-03,  ..., -1.1083e-03,\n",
      "            9.1696e-04,  1.0283e-03],\n",
      "          [ 3.7520e-04, -8.2609e-05, -2.0687e-04,  ..., -1.3707e-03,\n",
      "           -1.5310e-03,  4.3543e-04]],\n",
      "\n",
      "         [[-6.9674e-04, -5.4471e-04, -1.2607e-03,  ...,  5.4750e-07,\n",
      "            5.0423e-04, -1.1871e-04],\n",
      "          [-7.4704e-04, -7.5142e-05, -1.3202e-03,  ...,  5.7213e-05,\n",
      "           -4.0194e-04,  7.5270e-04],\n",
      "          [-2.4293e-04,  5.5776e-04, -9.6006e-05,  ..., -1.4510e-03,\n",
      "           -3.7615e-04, -5.9297e-04],\n",
      "          ...,\n",
      "          [-3.1320e-04,  4.0460e-04, -9.6139e-04,  ...,  1.3067e-03,\n",
      "            6.9410e-04, -3.5414e-04],\n",
      "          [-2.0358e-05, -7.2804e-04,  6.7421e-04,  ...,  4.3855e-04,\n",
      "            4.9294e-04,  1.1338e-03],\n",
      "          [ 1.2424e-05, -2.5804e-04,  7.3984e-04,  ..., -5.4489e-04,\n",
      "            1.8502e-04, -3.4738e-04]],\n",
      "\n",
      "         [[-4.8717e-04,  1.1689e-03, -1.1386e-03,  ..., -2.9089e-04,\n",
      "            2.0036e-05, -9.4187e-04],\n",
      "          [-1.0516e-03,  1.1672e-03, -5.9728e-04,  ..., -7.9061e-04,\n",
      "            1.0250e-03,  3.6858e-04],\n",
      "          [-9.9019e-04,  3.6038e-04, -1.0477e-04,  ...,  8.9584e-04,\n",
      "            8.8349e-04,  3.5369e-04],\n",
      "          ...,\n",
      "          [ 8.4240e-04,  1.9406e-06, -3.5167e-05,  ...,  5.0239e-04,\n",
      "           -1.8844e-04,  1.7598e-03],\n",
      "          [ 1.0253e-03, -2.9615e-04,  1.2171e-03,  ..., -8.6105e-04,\n",
      "           -4.1162e-04,  1.0952e-03],\n",
      "          [-1.1774e-03,  5.1582e-04, -1.0520e-03,  ...,  5.3052e-04,\n",
      "            1.8272e-03,  9.0332e-04]]]])\n"
     ]
    }
   ],
   "source": [
    "# Check grad after backward (should have values)\n",
    "print(list(model.parameters())[0].grad)  # Tensor with gradient values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e268712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0.95\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=1e-2,\n",
    "    momentum=0.95\n",
    ")\n",
    "optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f25aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77cb2c6",
   "metadata": {},
   "source": [
    "# differentiation in autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd62f76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4.], requires_grad=True), tensor([5., 7.], requires_grad=True))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([3., 4.], requires_grad=True)\n",
    "b = torch.tensor([5., 7.], requires_grad=True)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c150b",
   "metadata": {},
   "source": [
    "when we add the requires_grad function, looks like we are adding the ability of simply *tracking the operations on the tensors* via the computation graph in pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06dd54d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 56., 143.], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 3*a**3 - b**2\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1312dc",
   "metadata": {},
   "source": [
    "if we have `a` and `b` be the parameters of the NN, and `Q` to be the error, then we want the gradients of the errors to be the partial derivatives with respect to the parameters. Calling something like `.backward()` on `Q`, the autograd function will automatically calculate the partial derivatives (graidents and store them in the tensor's `.grad` attribute). a specific `gradient` argument is neeeded in `Q.backward()` since it is a vector. We can also aggregate `Q` into a scalar using `.sum()`, then pass the backward argument on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2fa9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient = external_grad) # looks like every tensor has a backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba8abe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 4.], requires_grad=True),\n",
       " tensor([5., 7.], requires_grad=True),\n",
       " tensor([ 56., 143.], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be09a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 81., 144.]), tensor([-10., -14.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f07f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 81., 144.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9*a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d4722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10., -14.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e6586",
   "metadata": {},
   "source": [
    "# vector calculus with `autograd`\n",
    "\n",
    "\n",
    "mathmatically speaking, `torch.autograd` is just an engine for computing the vector-Jacobian product. Mathmatically, it is $$J^T \\cdot \\overrightarrow{v}$$\n",
    "\n",
    "now if we take it another step furtther, if $\\overrightarrow{v}$ just so happens to be the gradient of a scalar function $l = g(\\overrightarrow{y})$, then bythe chain rule, the vector jacobian product would be the gradient of $l$ with respect to $\\overrightarrow{x}$. in the example above, `external_grad` represents `\\overrightarrow{v}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ef8de",
   "metadata": {},
   "source": [
    "## computational graphs \n",
    "\n",
    "you can understand autograd as something that keeps a record of data/tensors and all executed operations in a DAG, consisting of function objects. the leaves are the input tensors, roots are the output tensors. trace the graph from the root to the leaves, and you can get the gradients using the chain rule. \n",
    "\n",
    "In the forward pass, `autograd` does two things:\n",
    "\n",
    "- run the operation to compute a resulting tensor\n",
    "- maintain the **gradient function** of the operation in the DAG\n",
    "\n",
    "the backward pass then kicks off when you call `.backward()` (called on the DAG root). `autograd` will then\n",
    "\n",
    "- compute the gradients from each `.grad_fn` (grad function)\n",
    "- accumulate them in the respective tensors's `.grad` attribute, and \n",
    "- use the chain rule to propagate everything back to the leaf tensors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338f11b",
   "metadata": {},
   "source": [
    "\n",
    "Visually:\n",
    "\n",
    "![image.png](figures/dag_autograd.png)\n",
    "\n",
    "> DAGs are dynamic in pytorch!  this means that tyhe graph is recreated from scratch after each `.backward()` cal. there is a new graph created every time. this allows you to control the flow statements in the model and allows you to change the shape, size, and operations at each iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036cfc92",
   "metadata": {},
   "source": [
    "## exclusion from DAG\n",
    "\n",
    "if the `requires_grad` is set to False, then it's goig to be excluded from the DAG. this is how you freeze parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ec09b",
   "metadata": {},
   "source": [
    "in this resnet example, you freeze the parameters by excluding all of the parameters from the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da296b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fcd204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "Layer: 1\n",
      "Layer: 2\n",
      "Layer: 3\n",
      "Layer: 4\n",
      "Layer: 5\n",
      "Layer: 6\n",
      "Layer: 7\n",
      "Layer: 8\n",
      "Layer: 9\n",
      "Layer: 10\n",
      "Layer: 11\n",
      "Layer: 12\n",
      "Layer: 13\n",
      "Layer: 14\n",
      "Layer: 15\n",
      "Layer: 16\n",
      "Layer: 17\n",
      "Layer: 18\n",
      "Layer: 19\n",
      "Layer: 20\n",
      "Layer: 21\n",
      "Layer: 22\n",
      "Layer: 23\n",
      "Layer: 24\n",
      "Layer: 25\n",
      "Layer: 26\n",
      "Layer: 27\n",
      "Layer: 28\n",
      "Layer: 29\n",
      "Layer: 30\n",
      "Layer: 31\n",
      "Layer: 32\n",
      "Layer: 33\n",
      "Layer: 34\n",
      "Layer: 35\n",
      "Layer: 36\n",
      "Layer: 37\n",
      "Layer: 38\n",
      "Layer: 39\n",
      "Layer: 40\n",
      "Layer: 41\n",
      "Layer: 42\n",
      "Layer: 43\n",
      "Layer: 44\n",
      "Layer: 45\n",
      "Layer: 46\n",
      "Layer: 47\n",
      "Layer: 48\n",
      "Layer: 49\n",
      "Layer: 50\n",
      "Layer: 51\n",
      "Layer: 52\n",
      "Layer: 53\n",
      "Layer: 54\n",
      "Layer: 55\n",
      "Layer: 56\n",
      "Layer: 57\n",
      "Layer: 58\n",
      "Layer: 59\n",
      "Layer: 60\n",
      "Layer: 61\n"
     ]
    }
   ],
   "source": [
    "for idx, param in enumerate(model.parameters()):\n",
    "    print(f\"Layer: {idx}\")\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6727cca",
   "metadata": {},
   "source": [
    "then say we want ot finetune this model on a dataset with 10 new labels. for this particular model, the classifier is the last linear layer (`model.fc`). we can just replace it with a new linear layer (unfrozen by defaul that is the classifier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "757192e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original layer: Linear(in_features=512, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"original layer: {model.fc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8533ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated layer: Linear(in_features=512, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model.fc = nn.Linear(512, 10)\n",
    "print(f\"updated layer: {model.fc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c356c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "013cc5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 10)\n",
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27a1bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63763d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
